{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e54ecb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "\n",
    "\n",
    "import json \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56b15727",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27eb2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    responses.append(intent['responses'])\n",
    "        \n",
    "# using an if here because tags can repeat    \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "        \n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a751a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82ae0a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the target labels into a model understandable form.\n",
    "# “LabelEncoder()” function provided by scikit-learn\n",
    "lbl_encoder = LabelEncoder()\n",
    "lbl_encoder.fit(training_labels)\n",
    "training_labels = lbl_encoder.transform(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b639dc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 3, 3, 3, 7, 7, 7, 7, 0, 0, 0, 6, 6, 6, 5, 5, 5, 5,\n",
       "       5, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bea35404",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000 # the number of words to keep, based on frequency\n",
    "# why we limit vocab size?\n",
    "# we don’t want every unique token in our vocabulary. If it doesn’t appear at \n",
    "# least twice then might just be a spelling mistake or a \n",
    "# word we can’t learn anything about it if it doesn’t appear that often.\n",
    "\n",
    "# we might think that low frequency words (especially hapax legomena) \n",
    "# don't tell us very much useful information! And practically, this is often true; you don't \n",
    "# benefit from increasing the vocab size past a certain point, \n",
    "# but you will continue to incur increasing memory and performance costs associated with a larger input representation.\n",
    "\n",
    "embedding_dim = 16\n",
    "max_len = 20\n",
    "oov_token = \"<OOV>\" # oov_token is a value for “out of token”\n",
    "\n",
    "# Tokenization is a method to segregate a particular text into small chunks or tokens. \n",
    "# Here the tokens or chunks can be anything from words to characters, even subwords.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "\n",
    "# fitting the tokenizer on training sentences\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "# the index of the words in tokenizer. this is a dict\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# texts_to_sequences method in tokenizer class helps in converting tokens of text corpus into a sequence of integers.\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "\n",
    "# The “pad_sequences” method is used to make all the training text sequences into the same size.\n",
    "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef46b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd5b3222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 16)            16000     \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,680\n",
      "Trainable params: 16,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model training using sequential model of keras\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d916344e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0787 - accuracy: 0.09 - 0s 3ms/step - loss: 2.0792 - accuracy: 0.0909\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0776 - accuracy: 0.15 - 0s 3ms/step - loss: 2.0778 - accuracy: 0.1515\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0771 - accuracy: 0.21 - 0s 3ms/step - loss: 2.0772 - accuracy: 0.2121\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0767 - accuracy: 0.25 - 0s 2ms/step - loss: 2.0769 - accuracy: 0.2424\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0772 - accuracy: 0.21 - 0s 3ms/step - loss: 2.0767 - accuracy: 0.2424\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0761 - accuracy: 0.21 - 0s 2ms/step - loss: 2.0764 - accuracy: 0.2121\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0758 - accuracy: 0.21 - 0s 3ms/step - loss: 2.0757 - accuracy: 0.2121\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0745 - accuracy: 0.21 - 0s 3ms/step - loss: 2.0749 - accuracy: 0.2121\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0741 - accuracy: 0.21 - 0s 3ms/step - loss: 2.0741 - accuracy: 0.2121\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0729 - accuracy: 0.21 - 0s 3ms/step - loss: 2.0734 - accuracy: 0.2121\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0723 - accuracy: 0.37 - 0s 3ms/step - loss: 2.0728 - accuracy: 0.3636\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0727 - accuracy: 0.25 - 0s 4ms/step - loss: 2.0726 - accuracy: 0.2727\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0723 - accuracy: 0.28 - 0s 3ms/step - loss: 2.0721 - accuracy: 0.2727\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0719 - accuracy: 0.25 - 0s 3ms/step - loss: 2.0712 - accuracy: 0.2727\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0703 - accuracy: 0.31 - 0s 3ms/step - loss: 2.0707 - accuracy: 0.3030\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0709 - accuracy: 0.28 - 0s 2ms/step - loss: 2.0704 - accuracy: 0.2727\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0700 - accuracy: 0.28 - 0s 3ms/step - loss: 2.0702 - accuracy: 0.2727\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0698 - accuracy: 0.25 - 0s 3ms/step - loss: 2.0699 - accuracy: 0.2424\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0704 - accuracy: 0.28 - 0s 3ms/step - loss: 2.0693 - accuracy: 0.3030\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0683 - accuracy: 0.31 - 0s 2ms/step - loss: 2.0685 - accuracy: 0.3030\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0670 - accuracy: 0.28 - 0s 2ms/step - loss: 2.0677 - accuracy: 0.2727\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0679 - accuracy: 0.25 - 0s 3ms/step - loss: 2.0667 - accuracy: 0.2727\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0657 - accuracy: 0.31 - 0s 2ms/step - loss: 2.0656 - accuracy: 0.3333\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0638 - accuracy: 0.34 - 0s 3ms/step - loss: 2.0646 - accuracy: 0.3333\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0641 - accuracy: 0.43 - 0s 2ms/step - loss: 2.0636 - accuracy: 0.4242\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0631 - accuracy: 0.43 - 0s 2ms/step - loss: 2.0626 - accuracy: 0.4242\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0619 - accuracy: 0.46 - 0s 2ms/step - loss: 2.0617 - accuracy: 0.4848\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0612 - accuracy: 0.46 - 0s 3ms/step - loss: 2.0608 - accuracy: 0.4848\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0585 - accuracy: 0.46 - 0s 2ms/step - loss: 2.0600 - accuracy: 0.4545\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0592 - accuracy: 0.43 - 0s 3ms/step - loss: 2.0590 - accuracy: 0.4545\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0577 - accuracy: 0.46 - 0s 3ms/step - loss: 2.0582 - accuracy: 0.4545\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0588 - accuracy: 0.46 - 0s 3ms/step - loss: 2.0575 - accuracy: 0.4848\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0558 - accuracy: 0.43 - 0s 3ms/step - loss: 2.0564 - accuracy: 0.4242\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0547 - accuracy: 0.40 - 0s 2ms/step - loss: 2.0552 - accuracy: 0.3939\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0525 - accuracy: 0.40 - 0s 2ms/step - loss: 2.0543 - accuracy: 0.3939\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0522 - accuracy: 0.40 - 0s 2ms/step - loss: 2.0534 - accuracy: 0.3939\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0517 - accuracy: 0.40 - 0s 2ms/step - loss: 2.0525 - accuracy: 0.3939\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0512 - accuracy: 0.50 - 0s 3ms/step - loss: 2.0520 - accuracy: 0.4848\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0520 - accuracy: 0.46 - 0s 2ms/step - loss: 2.0516 - accuracy: 0.4848\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0531 - accuracy: 0.43 - 0s 3ms/step - loss: 2.0513 - accuracy: 0.4545\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0495 - accuracy: 0.46 - 0s 3ms/step - loss: 2.0503 - accuracy: 0.4545\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0473 - accuracy: 0.46 - 0s 2ms/step - loss: 2.0485 - accuracy: 0.4545\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0473 - accuracy: 0.46 - 0s 3ms/step - loss: 2.0471 - accuracy: 0.4848\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0460 - accuracy: 0.50 - 0s 2ms/step - loss: 2.0458 - accuracy: 0.4848\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0441 - accuracy: 0.50 - 0s 5ms/step - loss: 2.0445 - accuracy: 0.4848\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0430 - accuracy: 0.46 - 0s 4ms/step - loss: 2.0431 - accuracy: 0.4848\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0413 - accuracy: 0.50 - 0s 3ms/step - loss: 2.0414 - accuracy: 0.4848\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0395 - accuracy: 0.50 - 0s 2ms/step - loss: 2.0403 - accuracy: 0.4848\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0365 - accuracy: 0.53 - 0s 3ms/step - loss: 2.0392 - accuracy: 0.5152\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0377 - accuracy: 0.53 - 0s 2ms/step - loss: 2.0380 - accuracy: 0.5152\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0369 - accuracy: 0.50 - 0s 3ms/step - loss: 2.0367 - accuracy: 0.5152\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0329 - accuracy: 0.53 - 0s 2ms/step - loss: 2.0355 - accuracy: 0.5152\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0337 - accuracy: 0.50 - 0s 2ms/step - loss: 2.0342 - accuracy: 0.4848\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0356 - accuracy: 0.40 - 0s 3ms/step - loss: 2.0332 - accuracy: 0.4242\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0301 - accuracy: 0.46 - 0s 3ms/step - loss: 2.0316 - accuracy: 0.4545\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0316 - accuracy: 0.43 - 0s 3ms/step - loss: 2.0292 - accuracy: 0.4545\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0275 - accuracy: 0.46 - 0s 2ms/step - loss: 2.0272 - accuracy: 0.4848\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0257 - accuracy: 0.46 - 0s 3ms/step - loss: 2.0254 - accuracy: 0.4848\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 2.0225 - accuracy: 0.46 - 0s 3ms/step - loss: 2.0239 - accuracy: 0.4545\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0219 - accuracy: 0.37 - 0s 3ms/step - loss: 2.0223 - accuracy: 0.3636\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0201 - accuracy: 0.34 - 0s 3ms/step - loss: 2.0201 - accuracy: 0.3636\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0180 - accuracy: 0.37 - 0s 2ms/step - loss: 2.0175 - accuracy: 0.3636\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0144 - accuracy: 0.37 - 0s 4ms/step - loss: 2.0150 - accuracy: 0.3636\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0086 - accuracy: 0.37 - 0s 2ms/step - loss: 2.0126 - accuracy: 0.3636\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0089 - accuracy: 0.37 - 0s 2ms/step - loss: 2.0100 - accuracy: 0.3636\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0086 - accuracy: 0.34 - 0s 3ms/step - loss: 2.0079 - accuracy: 0.3636\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0079 - accuracy: 0.34 - 0s 2ms/step - loss: 2.0066 - accuracy: 0.3636\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0028 - accuracy: 0.37 - 0s 2ms/step - loss: 2.0043 - accuracy: 0.3636\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.0011 - accuracy: 0.37 - 0s 3ms/step - loss: 2.0015 - accuracy: 0.3636\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9974 - accuracy: 0.37 - 0s 2ms/step - loss: 1.9989 - accuracy: 0.3636\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9948 - accuracy: 0.37 - 0s 2ms/step - loss: 1.9964 - accuracy: 0.3636\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9893 - accuracy: 0.37 - 0s 2ms/step - loss: 1.9940 - accuracy: 0.3636\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9923 - accuracy: 0.37 - 0s 2ms/step - loss: 1.9918 - accuracy: 0.3636\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9892 - accuracy: 0.37 - 0s 3ms/step - loss: 1.9897 - accuracy: 0.3636\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9929 - accuracy: 0.34 - 0s 2ms/step - loss: 1.9873 - accuracy: 0.3636\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9838 - accuracy: 0.37 - 0s 2ms/step - loss: 1.9847 - accuracy: 0.3636\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9815 - accuracy: 0.34 - 0s 3ms/step - loss: 1.9817 - accuracy: 0.3636\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9768 - accuracy: 0.40 - 0s 2ms/step - loss: 1.9784 - accuracy: 0.3939\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9699 - accuracy: 0.40 - 0s 2ms/step - loss: 1.9751 - accuracy: 0.3939\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9669 - accuracy: 0.40 - 0s 3ms/step - loss: 1.9720 - accuracy: 0.3939\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9691 - accuracy: 0.40 - 0s 3ms/step - loss: 1.9695 - accuracy: 0.4242\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9673 - accuracy: 0.46 - 0s 3ms/step - loss: 1.9675 - accuracy: 0.4848\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9651 - accuracy: 0.46 - 0s 3ms/step - loss: 1.9652 - accuracy: 0.4848\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9578 - accuracy: 0.50 - 0s 3ms/step - loss: 1.9628 - accuracy: 0.4848\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9605 - accuracy: 0.46 - 0s 3ms/step - loss: 1.9600 - accuracy: 0.4848\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9626 - accuracy: 0.46 - 0s 3ms/step - loss: 1.9565 - accuracy: 0.4848\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9505 - accuracy: 0.50 - 0s 2ms/step - loss: 1.9516 - accuracy: 0.4848\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9466 - accuracy: 0.46 - 0s 2ms/step - loss: 1.9465 - accuracy: 0.4848\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9423 - accuracy: 0.43 - 0s 2ms/step - loss: 1.9423 - accuracy: 0.4545\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9360 - accuracy: 0.46 - 0s 2ms/step - loss: 1.9381 - accuracy: 0.4545\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9347 - accuracy: 0.43 - 0s 2ms/step - loss: 1.9339 - accuracy: 0.4545\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9271 - accuracy: 0.50 - 0s 2ms/step - loss: 1.9299 - accuracy: 0.4848\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9246 - accuracy: 0.50 - 0s 3ms/step - loss: 1.9258 - accuracy: 0.4848\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9206 - accuracy: 0.50 - 0s 2ms/step - loss: 1.9219 - accuracy: 0.4848\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9176 - accuracy: 0.43 - 0s 2ms/step - loss: 1.9175 - accuracy: 0.4545\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9134 - accuracy: 0.46 - 0s 2ms/step - loss: 1.9126 - accuracy: 0.4545\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9061 - accuracy: 0.46 - 0s 3ms/step - loss: 1.9080 - accuracy: 0.4545\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.9013 - accuracy: 0.46 - 0s 2ms/step - loss: 1.9032 - accuracy: 0.4545\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8956 - accuracy: 0.50 - 0s 2ms/step - loss: 1.8985 - accuracy: 0.4848\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8894 - accuracy: 0.50 - 0s 2ms/step - loss: 1.8943 - accuracy: 0.4848\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8901 - accuracy: 0.50 - 0s 2ms/step - loss: 1.8894 - accuracy: 0.4848\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8769 - accuracy: 0.46 - 0s 2ms/step - loss: 1.8855 - accuracy: 0.4545\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8825 - accuracy: 0.40 - 0s 2ms/step - loss: 1.8825 - accuracy: 0.4242\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8779 - accuracy: 0.37 - 0s 2ms/step - loss: 1.8796 - accuracy: 0.3636\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8815 - accuracy: 0.40 - 0s 2ms/step - loss: 1.8749 - accuracy: 0.4242\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8675 - accuracy: 0.46 - 0s 2ms/step - loss: 1.8694 - accuracy: 0.4545\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8638 - accuracy: 0.43 - 0s 2ms/step - loss: 1.8642 - accuracy: 0.4545\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8614 - accuracy: 0.46 - 0s 2ms/step - loss: 1.8583 - accuracy: 0.4545\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8485 - accuracy: 0.50 - 0s 3ms/step - loss: 1.8524 - accuracy: 0.4848\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8503 - accuracy: 0.50 - 0s 2ms/step - loss: 1.8471 - accuracy: 0.4848\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8388 - accuracy: 0.50 - 0s 2ms/step - loss: 1.8421 - accuracy: 0.4848\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8352 - accuracy: 0.50 - 0s 3ms/step - loss: 1.8374 - accuracy: 0.4848\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8318 - accuracy: 0.50 - 0s 2ms/step - loss: 1.8335 - accuracy: 0.4848\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8293 - accuracy: 0.50 - 0s 3ms/step - loss: 1.8302 - accuracy: 0.4848\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8262 - accuracy: 0.46 - 0s 3ms/step - loss: 1.8270 - accuracy: 0.4545\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8233 - accuracy: 0.43 - 0s 3ms/step - loss: 1.8240 - accuracy: 0.4545\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 1.8188 - accuracy: 0.43 - 0s 3ms/step - loss: 1.8195 - accuracy: 0.4545\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8142 - accuracy: 0.43 - 0s 4ms/step - loss: 1.8133 - accuracy: 0.4545\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8006 - accuracy: 0.50 - 0s 3ms/step - loss: 1.8061 - accuracy: 0.4848\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8107 - accuracy: 0.46 - 0s 3ms/step - loss: 1.7975 - accuracy: 0.4848\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8008 - accuracy: 0.46 - 0s 3ms/step - loss: 1.7862 - accuracy: 0.4848\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7920 - accuracy: 0.46 - 0s 3ms/step - loss: 1.7762 - accuracy: 0.4848\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7645 - accuracy: 0.53 - 0s 3ms/step - loss: 1.7702 - accuracy: 0.5152\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7609 - accuracy: 0.56 - 0s 3ms/step - loss: 1.7670 - accuracy: 0.5455\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7632 - accuracy: 0.40 - 0s 3ms/step - loss: 1.7627 - accuracy: 0.4242\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7572 - accuracy: 0.40 - 0s 2ms/step - loss: 1.7570 - accuracy: 0.4242\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7344 - accuracy: 0.40 - 0s 3ms/step - loss: 1.7506 - accuracy: 0.3939\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7444 - accuracy: 0.34 - 0s 3ms/step - loss: 1.7420 - accuracy: 0.3636\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7301 - accuracy: 0.37 - 0s 3ms/step - loss: 1.7329 - accuracy: 0.3636\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7192 - accuracy: 0.40 - 0s 3ms/step - loss: 1.7221 - accuracy: 0.3939\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6964 - accuracy: 0.46 - 0s 3ms/step - loss: 1.7129 - accuracy: 0.4545\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7110 - accuracy: 0.50 - 0s 2ms/step - loss: 1.7080 - accuracy: 0.4848\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7053 - accuracy: 0.46 - 0s 3ms/step - loss: 1.7044 - accuracy: 0.4848\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6960 - accuracy: 0.50 - 0s 3ms/step - loss: 1.7009 - accuracy: 0.4848\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6984 - accuracy: 0.46 - 0s 3ms/step - loss: 1.6969 - accuracy: 0.4848\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7041 - accuracy: 0.46 - 0s 3ms/step - loss: 1.6922 - accuracy: 0.4848\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6963 - accuracy: 0.46 - 0s 3ms/step - loss: 1.6821 - accuracy: 0.4848\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6711 - accuracy: 0.46 - 0s 3ms/step - loss: 1.6722 - accuracy: 0.4848\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6450 - accuracy: 0.50 - 0s 3ms/step - loss: 1.6634 - accuracy: 0.4848\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6782 - accuracy: 0.43 - 0s 3ms/step - loss: 1.6573 - accuracy: 0.4545\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6511 - accuracy: 0.43 - 0s 3ms/step - loss: 1.6548 - accuracy: 0.4242\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6437 - accuracy: 0.43 - 0s 4ms/step - loss: 1.6503 - accuracy: 0.4242\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6411 - accuracy: 0.46 - 0s 3ms/step - loss: 1.6440 - accuracy: 0.4545\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6387 - accuracy: 0.46 - 0s 4ms/step - loss: 1.6372 - accuracy: 0.4848\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6191 - accuracy: 0.50 - 0s 3ms/step - loss: 1.6301 - accuracy: 0.4848\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6266 - accuracy: 0.46 - 0s 3ms/step - loss: 1.6227 - accuracy: 0.4848\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6045 - accuracy: 0.50 - 0s 3ms/step - loss: 1.6159 - accuracy: 0.4848\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6087 - accuracy: 0.46 - 0s 2ms/step - loss: 1.6093 - accuracy: 0.4848\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.6018 - accuracy: 0.46 - 0s 2ms/step - loss: 1.6030 - accuracy: 0.4848\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5949 - accuracy: 0.50 - 0s 3ms/step - loss: 1.5977 - accuracy: 0.4848\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5818 - accuracy: 0.50 - 0s 3ms/step - loss: 1.5932 - accuracy: 0.4848\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5850 - accuracy: 0.50 - 0s 3ms/step - loss: 1.5883 - accuracy: 0.4848\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5838 - accuracy: 0.46 - 0s 2ms/step - loss: 1.5834 - accuracy: 0.4848\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5702 - accuracy: 0.50 - 0s 3ms/step - loss: 1.5792 - accuracy: 0.4848\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5676 - accuracy: 0.50 - 0s 3ms/step - loss: 1.5756 - accuracy: 0.4848\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5748 - accuracy: 0.46 - 0s 2ms/step - loss: 1.5729 - accuracy: 0.4848\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5951 - accuracy: 0.46 - 0s 3ms/step - loss: 1.5683 - accuracy: 0.4848\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5894 - accuracy: 0.46 - 0s 3ms/step - loss: 1.5601 - accuracy: 0.4848\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5753 - accuracy: 0.46 - 0s 2ms/step - loss: 1.5502 - accuracy: 0.4848\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5450 - accuracy: 0.46 - 0s 3ms/step - loss: 1.5418 - accuracy: 0.4848\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5395 - accuracy: 0.46 - 0s 2ms/step - loss: 1.5358 - accuracy: 0.4848\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5305 - accuracy: 0.46 - 0s 2ms/step - loss: 1.5330 - accuracy: 0.4848\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5220 - accuracy: 0.50 - 0s 2ms/step - loss: 1.5278 - accuracy: 0.4848\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5156 - accuracy: 0.50 - 0s 2ms/step - loss: 1.5198 - accuracy: 0.4848\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5098 - accuracy: 0.46 - 0s 2ms/step - loss: 1.5109 - accuracy: 0.4848\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5017 - accuracy: 0.50 - 0s 3ms/step - loss: 1.5041 - accuracy: 0.4848\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4976 - accuracy: 0.50 - 0s 2ms/step - loss: 1.5016 - accuracy: 0.4848\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5026 - accuracy: 0.50 - 0s 2ms/step - loss: 1.5033 - accuracy: 0.4848\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5030 - accuracy: 0.50 - 0s 3ms/step - loss: 1.5081 - accuracy: 0.4848\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5401 - accuracy: 0.46 - 0s 2ms/step - loss: 1.5093 - accuracy: 0.4848\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.5088 - accuracy: 0.46 - 0s 2ms/step - loss: 1.5041 - accuracy: 0.4848\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4654 - accuracy: 0.50 - 0s 3ms/step - loss: 1.4938 - accuracy: 0.4848\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4832 - accuracy: 0.50 - 0s 2ms/step - loss: 1.4823 - accuracy: 0.4848\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4446 - accuracy: 0.56 - 0s 3ms/step - loss: 1.4723 - accuracy: 0.5455\n",
      "Epoch 175/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 1.4963 - accuracy: 0.53 - 0s 3ms/step - loss: 1.4640 - accuracy: 0.5455\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4888 - accuracy: 0.50 - 0s 2ms/step - loss: 1.4573 - accuracy: 0.5152\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4516 - accuracy: 0.53 - 0s 2ms/step - loss: 1.4536 - accuracy: 0.5152\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4556 - accuracy: 0.50 - 0s 3ms/step - loss: 1.4498 - accuracy: 0.5152\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4455 - accuracy: 0.50 - 0s 3ms/step - loss: 1.4461 - accuracy: 0.5152\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4256 - accuracy: 0.53 - 0s 3ms/step - loss: 1.4415 - accuracy: 0.5152\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4357 - accuracy: 0.50 - 0s 2ms/step - loss: 1.4359 - accuracy: 0.5152\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4664 - accuracy: 0.50 - 0s 2ms/step - loss: 1.4315 - accuracy: 0.5152\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4584 - accuracy: 0.50 - 0s 2ms/step - loss: 1.4281 - accuracy: 0.5152\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4312 - accuracy: 0.50 - 0s 2ms/step - loss: 1.4246 - accuracy: 0.5152\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4057 - accuracy: 0.53 - 0s 3ms/step - loss: 1.4202 - accuracy: 0.5152\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4008 - accuracy: 0.53 - 0s 2ms/step - loss: 1.4151 - accuracy: 0.5152\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4105 - accuracy: 0.50 - 0s 2ms/step - loss: 1.4107 - accuracy: 0.5152\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4049 - accuracy: 0.53 - 0s 3ms/step - loss: 1.4068 - accuracy: 0.5152\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4117 - accuracy: 0.50 - 0s 2ms/step - loss: 1.4035 - accuracy: 0.5152\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3907 - accuracy: 0.53 - 0s 2ms/step - loss: 1.4005 - accuracy: 0.5152\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3845 - accuracy: 0.53 - 0s 3ms/step - loss: 1.3978 - accuracy: 0.5152\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3967 - accuracy: 0.50 - 0s 2ms/step - loss: 1.3960 - accuracy: 0.5152\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3934 - accuracy: 0.50 - 0s 2ms/step - loss: 1.3910 - accuracy: 0.5152\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3853 - accuracy: 0.50 - 0s 3ms/step - loss: 1.3837 - accuracy: 0.5152\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3813 - accuracy: 0.50 - 0s 2ms/step - loss: 1.3778 - accuracy: 0.5152\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3700 - accuracy: 0.53 - 0s 2ms/step - loss: 1.3722 - accuracy: 0.5152\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3791 - accuracy: 0.50 - 0s 3ms/step - loss: 1.3667 - accuracy: 0.5152\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3980 - accuracy: 0.50 - 0s 2ms/step - loss: 1.3642 - accuracy: 0.5152\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3785 - accuracy: 0.50 - 0s 2ms/step - loss: 1.3622 - accuracy: 0.5152\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3553 - accuracy: 0.53 - 0s 3ms/step - loss: 1.3611 - accuracy: 0.5152\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3522 - accuracy: 0.53 - 0s 2ms/step - loss: 1.3572 - accuracy: 0.5152\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3521 - accuracy: 0.50 - 0s 3ms/step - loss: 1.3521 - accuracy: 0.5152\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3618 - accuracy: 0.50 - 0s 2ms/step - loss: 1.3488 - accuracy: 0.5152\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3805 - accuracy: 0.50 - 0s 2ms/step - loss: 1.3477 - accuracy: 0.5152\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3468 - accuracy: 0.50 - 0s 3ms/step - loss: 1.3455 - accuracy: 0.5152\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3207 - accuracy: 0.53 - 0s 2ms/step - loss: 1.3428 - accuracy: 0.5152\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3091 - accuracy: 0.53 - 0s 2ms/step - loss: 1.3364 - accuracy: 0.5152\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3320 - accuracy: 0.50 - 0s 3ms/step - loss: 1.3292 - accuracy: 0.5152\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3244 - accuracy: 0.50 - 0s 2ms/step - loss: 1.3246 - accuracy: 0.5152\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3549 - accuracy: 0.50 - 0s 2ms/step - loss: 1.3208 - accuracy: 0.5152\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3312 - accuracy: 0.50 - 0s 3ms/step - loss: 1.3168 - accuracy: 0.5152\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3485 - accuracy: 0.50 - 0s 2ms/step - loss: 1.3135 - accuracy: 0.5152\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3006 - accuracy: 0.53 - 0s 2ms/step - loss: 1.3118 - accuracy: 0.5152\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3231 - accuracy: 0.50 - 0s 3ms/step - loss: 1.3080 - accuracy: 0.5152\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2847 - accuracy: 0.53 - 0s 2ms/step - loss: 1.3028 - accuracy: 0.5152\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3280 - accuracy: 0.50 - 0s 2ms/step - loss: 1.2977 - accuracy: 0.5152\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2554 - accuracy: 0.53 - 0s 3ms/step - loss: 1.2936 - accuracy: 0.5152\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3051 - accuracy: 0.50 - 0s 2ms/step - loss: 1.2899 - accuracy: 0.5152\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2812 - accuracy: 0.53 - 0s 3ms/step - loss: 1.2862 - accuracy: 0.5152\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2874 - accuracy: 0.50 - 0s 3ms/step - loss: 1.2824 - accuracy: 0.5152\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2764 - accuracy: 0.53 - 0s 2ms/step - loss: 1.2798 - accuracy: 0.5152\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2818 - accuracy: 0.50 - 0s 2ms/step - loss: 1.2783 - accuracy: 0.5152\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2594 - accuracy: 0.53 - 0s 2ms/step - loss: 1.2776 - accuracy: 0.5152\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2693 - accuracy: 0.53 - 0s 2ms/step - loss: 1.2767 - accuracy: 0.5152\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2480 - accuracy: 0.53 - 0s 3ms/step - loss: 1.2756 - accuracy: 0.5152\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2765 - accuracy: 0.50 - 0s 2ms/step - loss: 1.2716 - accuracy: 0.5152\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2623 - accuracy: 0.53 - 0s 2ms/step - loss: 1.2660 - accuracy: 0.5152\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2807 - accuracy: 0.50 - 0s 3ms/step - loss: 1.2617 - accuracy: 0.5152\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2868 - accuracy: 0.50 - 0s 2ms/step - loss: 1.2579 - accuracy: 0.5152\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2720 - accuracy: 0.50 - 0s 2ms/step - loss: 1.2528 - accuracy: 0.5152\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2535 - accuracy: 0.50 - 0s 3ms/step - loss: 1.2478 - accuracy: 0.5152\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2473 - accuracy: 0.50 - 0s 3ms/step - loss: 1.2439 - accuracy: 0.5152\n",
      "Epoch 233/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 1.2498 - accuracy: 0.50 - 0s 3ms/step - loss: 1.2408 - accuracy: 0.5152\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2440 - accuracy: 0.50 - 0s 3ms/step - loss: 1.2376 - accuracy: 0.5152\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2635 - accuracy: 0.50 - 0s 2ms/step - loss: 1.2352 - accuracy: 0.5152\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2662 - accuracy: 0.50 - 0s 3ms/step - loss: 1.2333 - accuracy: 0.5152\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2654 - accuracy: 0.50 - 0s 2ms/step - loss: 1.2322 - accuracy: 0.5152\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2112 - accuracy: 0.53 - 0s 3ms/step - loss: 1.2321 - accuracy: 0.5152\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2223 - accuracy: 0.53 - 0s 3ms/step - loss: 1.2318 - accuracy: 0.5152\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2456 - accuracy: 0.50 - 0s 3ms/step - loss: 1.2280 - accuracy: 0.5152\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2148 - accuracy: 0.53 - 0s 3ms/step - loss: 1.2221 - accuracy: 0.5152\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1927 - accuracy: 0.53 - 0s 2ms/step - loss: 1.2169 - accuracy: 0.5152\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2216 - accuracy: 0.50 - 0s 3ms/step - loss: 1.2135 - accuracy: 0.5152\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2076 - accuracy: 0.53 - 0s 2ms/step - loss: 1.2134 - accuracy: 0.5152\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2430 - accuracy: 0.50 - 0s 2ms/step - loss: 1.2161 - accuracy: 0.5152\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2493 - accuracy: 0.50 - 0s 2ms/step - loss: 1.2169 - accuracy: 0.5152\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1757 - accuracy: 0.53 - 0s 2ms/step - loss: 1.2137 - accuracy: 0.5152\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2285 - accuracy: 0.50 - 0s 2ms/step - loss: 1.2078 - accuracy: 0.5152\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2328 - accuracy: 0.50 - 0s 3ms/step - loss: 1.2005 - accuracy: 0.5152\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1891 - accuracy: 0.53 - 0s 2ms/step - loss: 1.1949 - accuracy: 0.5152\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2202 - accuracy: 0.50 - 0s 3ms/step - loss: 1.1907 - accuracy: 0.5152\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2037 - accuracy: 0.50 - 0s 2ms/step - loss: 1.1880 - accuracy: 0.5152\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1810 - accuracy: 0.53 - 0s 3ms/step - loss: 1.1859 - accuracy: 0.5152\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1600 - accuracy: 0.53 - 0s 3ms/step - loss: 1.1841 - accuracy: 0.5152\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1739 - accuracy: 0.53 - 0s 3ms/step - loss: 1.1823 - accuracy: 0.5152\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1689 - accuracy: 0.53 - 0s 3ms/step - loss: 1.1799 - accuracy: 0.5152\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1968 - accuracy: 0.50 - 0s 3ms/step - loss: 1.1765 - accuracy: 0.5152\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.2045 - accuracy: 0.50 - 0s 3ms/step - loss: 1.1725 - accuracy: 0.5152\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1462 - accuracy: 0.53 - 0s 2ms/step - loss: 1.1691 - accuracy: 0.5152\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1417 - accuracy: 0.53 - 0s 2ms/step - loss: 1.1664 - accuracy: 0.5152\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1580 - accuracy: 0.53 - 0s 3ms/step - loss: 1.1634 - accuracy: 0.5152\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1923 - accuracy: 0.50 - 0s 3ms/step - loss: 1.1610 - accuracy: 0.5152\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1678 - accuracy: 0.50 - 0s 2ms/step - loss: 1.1583 - accuracy: 0.5152\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1519 - accuracy: 0.53 - 0s 3ms/step - loss: 1.1555 - accuracy: 0.5152\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1144 - accuracy: 0.53 - 0s 3ms/step - loss: 1.1534 - accuracy: 0.5152\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1697 - accuracy: 0.50 - 0s 3ms/step - loss: 1.1519 - accuracy: 0.5152\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1131 - accuracy: 0.53 - 0s 2ms/step - loss: 1.1507 - accuracy: 0.5152\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1704 - accuracy: 0.50 - 0s 2ms/step - loss: 1.1501 - accuracy: 0.5152\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1396 - accuracy: 0.53 - 0s 3ms/step - loss: 1.1482 - accuracy: 0.5152\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1124 - accuracy: 0.53 - 0s 2ms/step - loss: 1.1469 - accuracy: 0.5152\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1691 - accuracy: 0.50 - 0s 2ms/step - loss: 1.1488 - accuracy: 0.5152\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1444 - accuracy: 0.50 - 0s 3ms/step - loss: 1.1473 - accuracy: 0.5152\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1354 - accuracy: 0.53 - 0s 3ms/step - loss: 1.1399 - accuracy: 0.5152\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1230 - accuracy: 0.53 - 0s 3ms/step - loss: 1.1290 - accuracy: 0.5152\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1227 - accuracy: 0.50 - 0s 2ms/step - loss: 1.1226 - accuracy: 0.5152\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1419 - accuracy: 0.50 - 0s 2ms/step - loss: 1.1251 - accuracy: 0.5152\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0978 - accuracy: 0.53 - 0s 2ms/step - loss: 1.1313 - accuracy: 0.5152\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1368 - accuracy: 0.50 - 0s 2ms/step - loss: 1.1342 - accuracy: 0.5152\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1513 - accuracy: 0.50 - 0s 3ms/step - loss: 1.1345 - accuracy: 0.5152\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1359 - accuracy: 0.50 - 0s 4ms/step - loss: 1.1326 - accuracy: 0.5152\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1267 - accuracy: 0.53 - 0s 3ms/step - loss: 1.1294 - accuracy: 0.5152\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1426 - accuracy: 0.50 - 0s 2ms/step - loss: 1.1255 - accuracy: 0.5152\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1240 - accuracy: 0.50 - 0s 2ms/step - loss: 1.1198 - accuracy: 0.5152\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1105 - accuracy: 0.53 - 0s 2ms/step - loss: 1.1134 - accuracy: 0.5152\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0949 - accuracy: 0.53 - 0s 2ms/step - loss: 1.1081 - accuracy: 0.5152\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1271 - accuracy: 0.50 - 0s 2ms/step - loss: 1.1012 - accuracy: 0.5152\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1153 - accuracy: 0.50 - 0s 2ms/step - loss: 1.0972 - accuracy: 0.5152\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0713 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0977 - accuracy: 0.5152\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0686 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0981 - accuracy: 0.5152\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0923 - accuracy: 0.53 - 0s 3ms/step - loss: 1.0973 - accuracy: 0.5152\n",
      "Epoch 291/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 1.0689 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0984 - accuracy: 0.5152\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1026 - accuracy: 0.50 - 0s 3ms/step - loss: 1.0995 - accuracy: 0.5152\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0666 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0966 - accuracy: 0.5152\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1056 - accuracy: 0.50 - 0s 2ms/step - loss: 1.0897 - accuracy: 0.5152\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1019 - accuracy: 0.50 - 0s 2ms/step - loss: 1.0814 - accuracy: 0.5152\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0713 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0746 - accuracy: 0.5152\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0485 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0718 - accuracy: 0.5152\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1005 - accuracy: 0.50 - 0s 3ms/step - loss: 1.0721 - accuracy: 0.5152\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0765 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0729 - accuracy: 0.5455\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0949 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0743 - accuracy: 0.5455\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0435 - accuracy: 0.56 - 0s 3ms/step - loss: 1.0749 - accuracy: 0.5455\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0437 - accuracy: 0.56 - 0s 2ms/step - loss: 1.0712 - accuracy: 0.5455\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0377 - accuracy: 0.56 - 0s 2ms/step - loss: 1.0646 - accuracy: 0.5455\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0520 - accuracy: 0.56 - 0s 3ms/step - loss: 1.0593 - accuracy: 0.5455\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0252 - accuracy: 0.56 - 0s 2ms/step - loss: 1.0546 - accuracy: 0.5455\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0767 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0509 - accuracy: 0.5455\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0429 - accuracy: 0.56 - 0s 3ms/step - loss: 1.0476 - accuracy: 0.5455\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0646 - accuracy: 0.50 - 0s 2ms/step - loss: 1.0473 - accuracy: 0.5152\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0284 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0502 - accuracy: 0.5152\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0808 - accuracy: 0.50 - 0s 3ms/step - loss: 1.0520 - accuracy: 0.5152\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0284 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0519 - accuracy: 0.5152\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0494 - accuracy: 0.53 - 0s 3ms/step - loss: 1.0498 - accuracy: 0.5152\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0207 - accuracy: 0.53 - 0s 3ms/step - loss: 1.0450 - accuracy: 0.5152\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0202 - accuracy: 0.53 - 0s 3ms/step - loss: 1.0398 - accuracy: 0.5152\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0640 - accuracy: 0.50 - 0s 3ms/step - loss: 1.0357 - accuracy: 0.5152\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0109 - accuracy: 0.56 - 0s 3ms/step - loss: 1.0326 - accuracy: 0.5455\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0071 - accuracy: 0.56 - 0s 2ms/step - loss: 1.0301 - accuracy: 0.5455\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0425 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0291 - accuracy: 0.5455\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0275 - accuracy: 0.56 - 0s 2ms/step - loss: 1.0277 - accuracy: 0.5455\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0161 - accuracy: 0.56 - 0s 2ms/step - loss: 1.0258 - accuracy: 0.5455\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0029 - accuracy: 0.56 - 0s 2ms/step - loss: 1.0228 - accuracy: 0.5455\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0339 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0205 - accuracy: 0.5455\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0321 - accuracy: 0.53 - 0s 3ms/step - loss: 1.0183 - accuracy: 0.5455\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0074 - accuracy: 0.59 - 0s 2ms/step - loss: 1.0157 - accuracy: 0.5758\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0401 - accuracy: 0.56 - 0s 2ms/step - loss: 1.0133 - accuracy: 0.5758\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0269 - accuracy: 0.56 - 0s 2ms/step - loss: 1.0110 - accuracy: 0.5758\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.9900 - accuracy: 0.59 - 0s 2ms/step - loss: 1.0091 - accuracy: 0.5758\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0030 - accuracy: 0.56 - 0s 2ms/step - loss: 1.0071 - accuracy: 0.5758\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.9870 - accuracy: 0.59 - 0s 3ms/step - loss: 1.0049 - accuracy: 0.5758\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0017 - accuracy: 0.59 - 0s 2ms/step - loss: 1.0033 - accuracy: 0.6061\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.0000 - accuracy: 0.59 - 0s 2ms/step - loss: 1.0013 - accuracy: 0.6061\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.9831 - accuracy: 0.62 - 0s 3ms/step - loss: 0.9994 - accuracy: 0.6061\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.9900 - accuracy: 0.65 - 0s 2ms/step - loss: 0.9981 - accuracy: 0.6364\n",
      "Epoch 334/500\n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fitting the model on the training labels and training sequences\n",
    "epochs = 500\n",
    "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d563505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chat_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# save all the required files in order to use it at the inference time. \n",
    "# we save the trained model, fitted tokenizer object and fitted label encoder object.\n",
    "\n",
    "# to save the trained model\n",
    "model.save(\"chat_model\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# to save the fitted tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# to save the fitted label encoder\n",
    "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
    "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b69e4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start messaging with the bot (type quit to stop)!\n",
      "User: hello\n",
      "ChatBot: Hi there\n",
      "User: who are you?\n",
      "ChatBot: I.m Joana, your bot assistant\n",
      "User: "
     ]
    }
   ],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import colorama \n",
    "colorama.init()\n",
    "from colorama import Fore, Style, Back\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "def chat():\n",
    "    # load trained model\n",
    "    model = keras.models.load_model('chat_model')\n",
    "\n",
    "    # load tokenizer object\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    # load label encoder object\n",
    "    with open('label_encoder.pickle', 'rb') as enc:\n",
    "        lbl_encoder = pickle.load(enc)\n",
    "\n",
    "    # parameters\n",
    "    max_len = 20\n",
    "    \n",
    "    while True:\n",
    "        print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
    "        inp = input()\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]),\n",
    "                                             truncating='post', maxlen=max_len))\n",
    "        tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
    "\n",
    "        for i in data['intents']:\n",
    "            if i['tag'] == tag:\n",
    "                print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL , np.random.choice(i['responses']))\n",
    "\n",
    "        # print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL,random.choice(responses))\n",
    "\n",
    "print(Fore.YELLOW + \"Start messaging with the bot (type quit to stop)!\" + Style.RESET_ALL)\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7652689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
